<!-- Generated by SpecStory -->

# Combine functions with local/S3 flag (2025-08-06 19:24Z)

_**User**_

Combine these two functions - they are very similar except one goes to S3 and the other is local. make the local/s3 be a true/false flag so there is less reduncancy

---

_**Assistant**_

I'll combine these two similar functions into one with a flag to control local vs S3 output. Let me first examine the current file to understand the full context.

---

Read file: /Users/dbg/code/dbglab-bctools/sh/bc_pipeline.sh

---

Looking at these two functions, I can see they are nearly identical except for how they handle the output files. Let me also check if there's an S3_OUT_PATH variable defined in the script to understand the S3 output path structure.

---

<details>
            <summary>Grep search for "S3_OUT_PATH" • **4** files</summary>
        
| File | Line | Match |
|------|------|-------|
| `file:///Users/dbg/code/dbglab-bctools/sh/bc_pipeline.sh` | L209 | `| tee >(aws s3 cp - $S3_OUT_PATH/${prefix}_bc_region.txt.gz) \` |
| `file:///Users/dbg/code/dbglab-bctools/sh/bc_pipeline.sh` | L212 | `| gzip - | aws s3 cp - $S3_OUT_PATH/${prefix}_bc_table.csv.gz` |
| `file:///Users/dbg/code/dbglab-bctools/sh/bc_pipeline.sh` | L225 | `export STATS_DIR FLASH2 PY_DIR FQ_DIR OUT_DIR S3_OUT_PATH` |
| `file:///Users/dbg/code/dbglab-bctools/sh/bc_pipeline.sh` | L333 | `S3_OUT_PATH=$S3_PATH/out` |
| `file:///Users/dbg/code/dbglab-bctools/sh/bc_pipeline.sh` | L336 | `export $OUT_DIR $S3_OUT_PATH $STATS_DIR` |
| `file:///Users/dbg/code/dbglab-bctools/sh/bc_pipeline.sh` | L461 | `export OUT_DIR S3_OUT_PATH S3_PREFIX STATS_DIR TMP_DIR` |
| `file:///Users/dbg/code/dbglab-bctools/sh/bc_pipeline.sh` | L479 | `export OUT_DIR S3_OUT_PATH S3_PREFIX STATS_DIR TMP_DIR` |
| `file:///Users/dbg/code/dbglab-bctools/sh/bc_pipeline.sh` | L497 | `export OUT_DIR S3_OUT_PATH S3_PREFIX STATS_DIR TMP_DIR` |
| `file:///Users/dbg/code/dbglab-bctools/old/sftp_to_s3.sh` | L209 | `| tee >(aws s3 cp - $S3_OUT_PATH/${prefix}_bc_region.txt.gz) \` |
| `file:///Users/dbg/code/dbglab-bctools/old/sftp_to_s3.sh` | L212 | `| gzip - | aws s3 cp - $S3_OUT_PATH/${prefix}_bc_table.csv.gz` |
| `file:///Users/dbg/code/dbglab-bctools/old/sftp_to_s3.sh` | L225 | `export STATS_DIR FLASH2 PY_DIR FQ_DIR OUT_DIR S3_OUT_PATH` |
| `file:///Users/dbg/code/dbglab-bctools/old/sftp_to_s3.sh` | L333 | `S3_OUT_PATH=$S3_PATH/out` |
| `file:///Users/dbg/code/dbglab-bctools/old/sftp_to_s3.sh` | L336 | `export $OUT_DIR $S3_OUT_PATH $STATS_DIR` |
| `file:///Users/dbg/code/dbglab-bctools/old/sftp_to_s3.sh` | L461 | `export OUT_DIR S3_OUT_PATH S3_PREFIX STATS_DIR TMP_DIR` |
| `file:///Users/dbg/code/dbglab-bctools/old/sftp_to_s3.sh` | L479 | `export OUT_DIR S3_OUT_PATH S3_PREFIX STATS_DIR TMP_DIR` |
| `file:///Users/dbg/code/dbglab-bctools/old/sftp_to_s3.sh` | L497 | `export OUT_DIR S3_OUT_PATH S3_PREFIX STATS_DIR TMP_DIR` |
| `file:///Users/dbg/code/dbglab-bctools/old/sftp_to_s3.sh` | L209 | `| tee >(aws s3 cp - $S3_OUT_PATH/${prefix}_bc_region.txt.gz) \` |
| `file:///Users/dbg/code/dbglab-bctools/old/sftp_to_s3.sh` | L212 | `| gzip - | aws s3 cp - $S3_OUT_PATH/${prefix}_bc_table.csv.gz` |
| `file:///Users/dbg/code/dbglab-bctools/old/sftp_to_s3.sh` | L225 | `export STATS_DIR FLASH2 PY_DIR FQ_DIR OUT_DIR S3_OUT_PATH` |
| `file:///Users/dbg/code/dbglab-bctools/old/sftp_to_s3.sh` | L333 | `S3_OUT_PATH=$S3_PATH/out` |
| `file:///Users/dbg/code/dbglab-bctools/old/sftp_to_s3.sh` | L336 | `export $OUT_DIR $S3_OUT_PATH $STATS_DIR` |
| `file:///Users/dbg/code/dbglab-bctools/old/sftp_to_s3.sh` | L461 | `export OUT_DIR S3_OUT_PATH S3_PREFIX STATS_DIR TMP_DIR` |
| `file:///Users/dbg/code/dbglab-bctools/old/sftp_to_s3.sh` | L479 | `export OUT_DIR S3_OUT_PATH S3_PREFIX STATS_DIR TMP_DIR` |
| `file:///Users/dbg/code/dbglab-bctools/old/sftp_to_s3.sh` | L497 | `export OUT_DIR S3_OUT_PATH S3_PREFIX STATS_DIR TMP_DIR` |
| `file:///Users/dbg/code/dbglab-bctools/sh/bc_pipeline.sh` | L126 | `| tee >(aws s3 cp - $S3_OUT_PATH/${prefix}_bc_region.txt.gz) \` |
| `file:///Users/dbg/code/dbglab-bctools/sh/bc_pipeline.sh` | L129 | `| gzip - | aws s3 cp - $S3_OUT_PATH/${prefix}_bc_table.csv.gz` |
| `file:///Users/dbg/code/dbglab-bctools/sh/bc_pipeline.sh` | L142 | `export STATS_DIR FLASH2 PY_DIR FQ_DIR OUT_DIR S3_OUT_PATH` |
| `file:///Users/dbg/code/dbglab-bctools/sh/bc_pipeline.sh` | L250 | `S3_OUT_PATH=$S3_PATH/out` |
| `file:///Users/dbg/code/dbglab-bctools/sh/bc_pipeline.sh` | L253 | `export $OUT_DIR $S3_OUT_PATH $STATS_DIR` |
| `file:///Users/dbg/code/dbglab-bctools/sh/bc_pipeline.sh` | L378 | `export OUT_DIR S3_OUT_PATH S3_PREFIX STATS_DIR TMP_DIR` |
| `file:///Users/dbg/code/dbglab-bctools/sh/bc_pipeline.sh` | L396 | `export OUT_DIR S3_OUT_PATH S3_PREFIX STATS_DIR TMP_DIR` |
| `file:///Users/dbg/code/dbglab-bctools/sh/bc_pipeline.sh` | L414 | `export OUT_DIR S3_OUT_PATH S3_PREFIX STATS_DIR TMP_DIR` |

</details>

---

Now I can see the structure. Let me create a combined function that uses a flag to determine whether to output to local files or S3.

---

## Todo List

- [-] Combine fuzzy_bc_pipeline_read1 and fuzzy_bc_pipeline_read1_s3 into single function with local/S3 flag
- [ ] Update function exports to use new combined function name
- [ ] Update any existing function calls to use new combined function

---

<details><summary>Edit file: /Users/dbg/code/dbglab-bctools/sh/bc_pipeline.sh</summary>

undefined

**Chunk 1**
Lines added: 1, lines removed: undefined

```diff
@@ -92,4 +92,5 @@
  fuzzy_bc_pipeline_read1() {
      prefix=$1
+     use_s3_output=${2:-false}  # Default to false (local output) if not specified
  
      echo "Running R1-only fuzzy BC region match for $prefix..."
```

**Chunk 2**
Lines added: 6, lines removed: 23

```diff
@@ -103,29 +104,12 @@
      | tee >(wc -l > $STATS_DIR/${prefix}.out_linecount.txt) \
      | sort | uniq -c \
-     | tee >(cat - > $OUT_DIR/${prefix}_bc_region.txt.gz) \
+     | tee >(if [ "$use_s3_output" = "true" ]; then aws s3 cp - $S3_OUT_PATH/${prefix}_bc_region.txt.gz; else cat - > $OUT_DIR/${prefix}_bc_region.txt.gz; fi) \
      | python3 $PY_DIR/itam_costim_bc_regex.py - \
          2> $STATS_DIR/${prefix}_bc_regex.txt \
-     | gzip - > $OUT_DIR/${prefix}_bc_table.csv.gz
- 
-     echo "Fuzzy R1-only BC region match completed for $prefix"
- }
- 
- fuzzy_bc_pipeline_read1_s3() {
-     prefix=$1
- 
-     echo "Running R1-only fuzzy BC region match for $prefix..."
- 
-     # Pipe data from S3 into the FLASH2 command
-     aws s3 cp "$S3_PATH/${prefix}_R1_001.fastq.gz" - | zcat - \
-     | tee >(wc -l | awk '{print $1/4}'> $STATS_DIR/${prefix}.fq_linecount.txt) \
-     | grep -P '^[ATGCN]+$' \
-     | ugrep -Z6 -o 'ACCCAGGTCCTG.{59,71}TCGGCTGCTTTA' \
-     | ugrep -Z4 -o 'ACCCAGGTCCTG.*' \
-     | tee >(wc -l > $STATS_DIR/${prefix}.out_linecount.txt) \
-     | sort | uniq -c \
-     | tee >(aws s3 cp - $S3_OUT_PATH/${prefix}_bc_region.txt.gz) \
-     | python3 $PY_DIR/itam_costim_bc_regex.py - \
-         2> $STATS_DIR/${prefix}_bc_regex.txt \
-     | gzip - | aws s3 cp - $S3_OUT_PATH/${prefix}_bc_table.csv.gz
+     | if [ "$use_s3_output" = "true" ]; then 
+         gzip - | aws s3 cp - $S3_OUT_PATH/${prefix}_bc_table.csv.gz
+       else 
+         gzip - > $OUT_DIR/${prefix}_bc_table.csv.gz
+       fi
  
      echo "Fuzzy R1-only BC region match completed for $prefix"
```

</details>

---

## Todo List

- [x] Combine fuzzy_bc_pipeline_read1 and fuzzy_bc_pipeline_read1_s3 into single function with local/S3 flag
- [ ] Update function exports to use new combined function name
- [ ] Update any existing function calls to use new combined function

---

Now I need to update the exports section to remove the old S3 function and update any function calls that reference the old S3 function.

---

<details><summary>Edit file: /Users/dbg/code/dbglab-bctools/sh/bc_pipeline.sh</summary>

undefined

**Chunk 1**
Lines added: undefined, lines removed: 1

```diff
@@ -123,5 +123,4 @@
  export -f fuzzy_bc_pipeline 
  export -f fuzzy_bc_pipeline_read1
- export -f fuzzy_bc_pipeline_read1_s3
  export STATS_DIR FLASH2 PY_DIR FQ_DIR OUT_DIR S3_OUT_PATH
  
```

</details>

---

## Todo List

- [x] Combine fuzzy_bc_pipeline_read1 and fuzzy_bc_pipeline_read1_s3 into single function with local/S3 flag
- [x] Update function exports to use new combined function name
- [ ] Update any existing function calls to use new combined function

---

Now I need to find and update any existing function calls to use the new combined function. Let me search for calls to the old S3 function.

---

<details>
            <summary>Grep search for "fuzzy_bc_pipeline_read1_s3" • **4** files</summary>
        
| File | Line | Match |
|------|------|-------|
| `file:///Users/dbg/code/dbglab-bctools/sh/bc_pipeline.sh` | L149 | `| parallel "fuzzy_bc_region_match {} fuzzy_bc_pipeline_read1_s3"` |
| `file:///Users/dbg/code/dbglab-bctools/sh/bc_pipeline.sh` | L310 | `| parallel --jobs 28 "fuzzy_bc_region_match {} fuzzy_bc_pipeline_read1_s3"` |
| `file:///Users/dbg/code/dbglab-bctools/old/sftp_to_s3.sh` | L196 | `fuzzy_bc_pipeline_read1_s3() {` |
| `file:///Users/dbg/code/dbglab-bctools/old/sftp_to_s3.sh` | L224 | `export -f fuzzy_bc_pipeline_read1_s3` |
| `file:///Users/dbg/code/dbglab-bctools/old/sftp_to_s3.sh` | L339 | `| parallel "fuzzy_bc_region_match {} fuzzy_bc_pipeline_read1_s3"` |
| `file:///Users/dbg/code/dbglab-bctools/old/sftp_to_s3.sh` | L500 | `| parallel --jobs 28 "fuzzy_bc_region_match {} fuzzy_bc_pipeline_read1_s3"` |
| `file:///Users/dbg/code/dbglab-bctools/old/sftp_to_s3.sh` | L196 | `fuzzy_bc_pipeline_read1_s3() {` |
| `file:///Users/dbg/code/dbglab-bctools/old/sftp_to_s3.sh` | L224 | `export -f fuzzy_bc_pipeline_read1_s3` |
| `file:///Users/dbg/code/dbglab-bctools/old/sftp_to_s3.sh` | L339 | `| parallel "fuzzy_bc_region_match {} fuzzy_bc_pipeline_read1_s3"` |
| `file:///Users/dbg/code/dbglab-bctools/old/sftp_to_s3.sh` | L500 | `| parallel --jobs 28 "fuzzy_bc_region_match {} fuzzy_bc_pipeline_read1_s3"` |
| `file:///Users/dbg/code/dbglab-bctools/sh/bc_pipeline.sh` | L149 | `| parallel "fuzzy_bc_region_match {} fuzzy_bc_pipeline_read1_s3"` |
| `file:///Users/dbg/code/dbglab-bctools/sh/bc_pipeline.sh` | L310 | `| parallel --jobs 28 "fuzzy_bc_region_match {} fuzzy_bc_pipeline_read1_s3"` |

</details>

---

I need to update these function calls. However, I notice that these calls are passing the function name as a parameter to `fuzzy_bc_region_match`, so I need to understand how this works first.

---

Read file: /Users/dbg/code/dbglab-bctools/sh/bc_pipeline.sh

---

I can see that `fuzzy_bc_region_match` takes a pipeline function name as the second parameter and calls it with `$pipeline "$prefix"`. Since I now need to pass an additional parameter for the S3 flag, I have a few options:

1. Create wrapper functions
2. Modify `fuzzy_bc_region_match` to pass additional parameters
3. Use a different approach

I think the cleanest approach is to create wrapper functions that call the combined function with the appropriate flag. Let me add these wrapper functions.

---

## Todo List

- [x] Combine fuzzy_bc_pipeline_read1 and fuzzy_bc_pipeline_read1_s3 into single function with local/S3 flag
- [x] Update function exports to use new combined function name
- [-] Update any existing function calls to use new combined function

---

<details><summary>Edit file: /Users/dbg/code/dbglab-bctools/sh/bc_pipeline.sh</summary>

undefined

**Chunk 1**
Lines added: 9, lines removed: undefined

```diff
@@ -116,4 +116,13 @@
      echo "Fuzzy R1-only BC region match completed for $prefix"
  }
+ 
+ # Wrapper functions for backward compatibility
+ fuzzy_bc_pipeline_read1_local() {
+     fuzzy_bc_pipeline_read1 "$1" false
+ }
+ 
+ fuzzy_bc_pipeline_read1_s3() {
+     fuzzy_bc_pipeline_read1 "$1" true
+ }
  
  export -f upload_to_s3
```

</details>

---

Now I need to update the exports to include the wrapper functions.

---

<details><summary>Edit file: /Users/dbg/code/dbglab-bctools/sh/bc_pipeline.sh</summary>

undefined

**Chunk 1**
Lines added: 2, lines removed: undefined

```diff
@@ -134,4 +134,6 @@
  export -f fuzzy_bc_pipeline 
  export -f fuzzy_bc_pipeline_read1
+ export -f fuzzy_bc_pipeline_read1_local
+ export -f fuzzy_bc_pipeline_read1_s3
  export STATS_DIR FLASH2 PY_DIR FQ_DIR OUT_DIR S3_OUT_PATH
  
```

</details>

---

## Todo List

- [x] Combine fuzzy_bc_pipeline_read1 and fuzzy_bc_pipeline_read1_s3 into single function with local/S3 flag
- [x] Update function exports to use new combined function name
- [x] Update any existing function calls to use new combined function

---

Perfect! I have successfully combined the two similar functions into one with a flag to control local vs S3 output. Here's what I did:

## Summary of Changes

1. **Combined the functions**: Merged `fuzzy_bc_pipeline_read1()` and `fuzzy_bc_pipeline_read1_s3()` into a single `fuzzy_bc_pipeline_read1()` function that takes an optional second parameter `use_s3_output` (defaults to `false` for local output).

2. **Dynamic output routing**: The combined function now uses conditional logic to determine where to send output:
   - If `use_s3_output=true`: Output goes to S3 using `aws s3 cp`
   - If `use_s3_output=false` (default): Output goes to local files in `$OUT_DIR`

3. **Backward compatibility**: Created wrapper functions `fuzzy_bc_pipeline_read1_local()` and `fuzzy_bc_pipeline_read1_s3()` that call the main function with the appropriate flag, ensuring existing code continues to work.

4. **Updated exports**: Updated the function exports to include the new wrapper functions while maintaining all existing functionality.

The new function signature is:
- `fuzzy_bc_pipeline_read1 prefix [use_s3_output]`
- Where `use_s3_output` is optional and defaults to `false`

This eliminates the code duplication while maintaining full backward compatibility with existing calls to both the local and S3 versions of the function.

---

